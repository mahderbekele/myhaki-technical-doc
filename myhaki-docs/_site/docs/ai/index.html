<h1>MyHaki Agent AI Integration</h1>
<p>MyHaki Agent is an AI-powered legal assistant designed to help users access, understand, and summarize legal information with ease. It uses a Retrieval-Augmented Generation (RAG) pipeline that combines Supabase vector search and Google Gemini to deliver accurate, context-aware answers from legal documents. The system processes user queries by retrieving relevant chunks from a vector database and generating responses grounded in Kenyan legal texts, ensuring reliability for pre-trial detainees and legal professionals.</p>
<hr>
<h1>AI Models</h1>
<h3>Legal-BERT</h3>
<p>Specialized BERT model for legal text understanding and embeddings generation (<code>nlpaueb/legal-bert-base-uncased</code>).</p>
<p><strong>How to Use:</strong></p>
<div class="api-block">
<pre class="api-dark">
from transformers import AutoTokenizer, AutoModel
import torch
<p>tokenizer = AutoTokenizer.from_pretrained(&quot;nlpaueb/legal-bert-base-uncased&quot;)
model = AutoModel.from_pretrained(&quot;nlpaueb/legal-bert-base-uncased&quot;)</p>
<p>inputs = tokenizer(text, return_tensors=&quot;pt&quot;, padding=True, truncation=True, max_length=512)</p>
<p>with torch.no_grad():
outputs = model(**inputs)</p>
<p>embeddings = outputs.last_hidden_state.mean(dim=1).numpy().tolist()</p>
</pre>
</div>
<p><strong>Apply:</strong> Embed legal chunks (e.g., court judgments) and store in pgvector; query similarity for RAG retrieval.</p>
<hr>
<h3>RAG Pipeline</h3>
<p>Retrieval-Augmented Generation for context-aware case processing (LangChain + Vector Search).</p>
<p><strong>How to Use:</strong></p>
<div class="api-block">
<pre class="api-dark">
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
<p>embeddings = HuggingFaceEmbeddings(model_name=&quot;nlpaueb/legal-bert-base-uncased&quot;)
vectorstore = Chroma.from_documents(docs, embeddings) # Load pre-embedded chunks</p>
<p>from langchain.chains import RetrievalQA
from langchain.llms import GoogleGenerativeAI</p>
<p>llm = GoogleGenerativeAI(model=&quot;gemini-2.5-flash&quot;)
qa_chain = RetrievalQA.from_chain_type(llm, retriever=vectorstore.as_retriever(search_kwargs={&quot;k&quot;: 5}))</p>
<p>result = qa_chain.run(&quot;Classify this case: [query text]&quot;) # Outputs classification/urgency</p>
</pre>
</div>
<p><strong>Apply:</strong> Chain with Gemini for JSON outputs (case_type, urgency); handle multilingual by prompt engineering.</p>
<hr>
<h3>Gemini LLM</h3>
<p>Advanced language model for classification and reasoning (Gemini 2.5 Flash).</p>
<p><strong>How to Use:</strong></p>
<div class="api-block">
<pre class="api-dark"> 
from google.generativeai import GenerativeModel, configure
import json
<p>configure(api_key=&quot;your_api_key&quot;)
model = GenerativeModel('gemini-2.5-flash')</p>
<p>prompt = &quot;Analyze query and context for case_type, urgency, reasoning in JSON.&quot;
response = model.generate_content(prompt)</p>
<p>result = json.loads(response.text) # Parse for {&quot;case_type&quot;: &quot;civil&quot;, &quot;urgency&quot;: &quot;high&quot;, &quot;reasoning&quot;: &quot;...&quot;}</p>
</pre>
</div>
<p><strong>Apply:</strong> Use in RAG for structured outputs; override urgency with date rules (e.g., trial &lt;15 days = &quot;urgent&quot;).</p>
<hr>
<h2>Data Pipeline Architecture</h2>
<p><strong>Ingestion → Processing → Delivery</strong></p>
<h3>Pipeline Stages (Detailed Usage):</h3>
<ul>
<li><strong>Collection:</strong> Raw case data intake via API or upload (e.g., CSV/PDF/JSON); use Pandas for initial parsing:</li>
</ul>
<div class="api-block">
<pre class="api-dark">
import pandas as pd
df = pd.read_csv("legal_cases.csv")
</pre>
</div>
<ul>
<li>
<p><strong>Cleaning:</strong> Data normalization with regex/BeautifulSoup for text extraction; remove duplicates, handle missing fields.</p>
</li>
<li>
<p><strong>Chunking:</strong> Text segmentation (page/sentence/character-level) using LangChain:</p>
</li>
</ul>
<div class="api-block">
<pre class="api-dark">
from langchain.text_splitter import RecursiveCharacterTextSplitter
<p>splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_text(text)</p>
</pre>
</div>
<ul>
<li>
<p><strong>Embedding:</strong> Vector generation with Legal-BERT (as above); batch process for efficiency.</p>
</li>
<li>
<p><strong>Storage:</strong> Insert into Supabase pgvector for similarity searches:</p>
</li>
</ul>
<div class="api-block">
<pre class="api-dark">
from supabase import create_client
<p>client = create_client(SUPABASE_URL, SUPABASE_KEY)
client.table(&quot;embeddings&quot;).insert({&quot;id&quot;: chunk_id, &quot;embedding&quot;: embedding}).execute()</p>
</pre>
</div>
<hr>
<h2>API Query Endpoint (FastAPI Example)</h2>
<div class="api-block">
<pre class="api-dark"> 
from fastapi import FastAPI
<p>app = FastAPI()</p>
<p>@app.post(&quot;/query&quot;)
def query_case(q: str):
results = vectorstore.similarity_search(q, k=5)
return {
&quot;chunks&quot;: results,
&quot;metadatas&quot;: [r.metadata for r in results]
}
</pre></p>
</div>
---
# Technologies
<h2>Technology Stack:</h2>
<ul>
<li><strong>LangChain:</strong><br>
Framework for RAG apps; chain retrievers/LLMs.<br>
Example: Use <code>RetrievalQA</code> to integrate Supabase vectorstore with Gemini.<div class="api-block">
</li>
</ul>
<pre class="api-dark">
qa_chain.run("Classify case: [query]") 
</pre>
</div>
<ul>
<li><strong>pgvector:</strong><br>
PostgreSQL vector similarity extension; query embeddings efficiently.<br>
Example:</li>
</ul>
<div class="api-block">
<pre class="api-dark">
SELECT * FROM embeddings ORDER BY embedding <=> query_embedding LIMIT 5;
</pre>
</div>
<p>Executed via Supabase RPC for scalable searches on legal chunks.</p>
<ul>
<li><strong>Hugging Face:</strong><br>
Model hub and transformers library.<br>
Example: Load Sentence Transformers for quick embedding generation.</li>
</ul>
<div class="api-block">
<pre class="api-dark">
from sentence_transformers import SentenceTransformer
import functools
<p>@functools.lru_cache()
def get_embedding_model():
return SentenceTransformer(&quot;sentence-transformers/all-mpnet-base-v2&quot;)
</pre></p>
</div>
---
<h1>Deployment Process</h1>
<h2>AI Agent Deployment (FastAPI)</h2>
<ul>
<li>
<p><strong>Platform:</strong> Google Cloud Platform (Cloud Run) for scalable, serverless hosting. Selected after Heroku’s space limits to manage variable LSK loads.</p>
</li>
<li>
<p><strong>Build:</strong> Dockerized FastAPI microservice providing endpoints (e.g., case query/classification), using stateless containers for fast scaling.</p>
</li>
<li>
<p><strong>Environment Variables:</strong> Securely managed in Google Cloud Secret Manager (e.g., <code>GEMINI_API_KEY</code>, <code>SUPABASE_URL</code>); loaded in code with <code>os.getenv</code> to avoid exposure.</p>
</li>
<li>
<p><strong>Scaling:</strong> Auto-scales via Cloud Run with 1Gi memory and port 8080; handles spikes from user submissions.</p>
</li>
<li>
<p><strong>Monitoring:</strong> Google Cloud Monitoring for logs and alerts (e.g., &gt;5s latency alerts); health check implemented with <code>curl</code> for uptime.</p>
</li>
</ul>
<hr>
<h2>Deployment Steps (Concise Usage):</h2>
<ul>
<li>
<p><strong>Push to Feature Branches/PRs:</strong> GitHub triggers reviews; merging to <code>main</code> activates deployment pipeline.</p>
</li>
<li>
<p><strong>GitHub Actions CI/CD:</strong> Verifies tests and lint, builds Docker image.</p>
</li>
</ul>
<p><strong>Sample YAML snippet:</strong></p>
<div class="api-block">
<pre class="api-dark">
name: Deploy FastAPI App
on:
push:
branches:
- feature/myhaki_predictive_agent
<p>jobs:
deploy:
runs-on: ubuntu-latest
steps:</p>
<ul>
<li>uses: actions/checkout@v4</li>
<li>uses: google-github-actions/auth@v2
with:
credentials_json: '$'</li>
<li>uses: google-github-actions/setup-gcloud@v2</li>
<li>run: |
gcloud run deploy myhaki-agent
--image docker.io/$:tag
--project &quot;$&quot;
--region europe-west1
--allow-unauthenticated
--port 8080
--memory 1Gi
--set-env-vars GEMINI_API_KEY=$,SUPABASE_URL=$</li>
</ul>
</pre>
</div>
<ul>
<li>
<p><strong>Explanation:</strong> Automates checkout, authentication, and deployment; pulls Docker image from Docker Hub; sets environment variables securely.</p>
</li>
<li>
<p><strong>Merge Triggers Deployment:</strong></p>
</li>
</ul>
<div class="api-block">
<pre class="api-dark">
  Build and push to Google Container Registry (GCR):  
docker tag myhaki-agent gcr.io/project-id/myhaki-agent:tag
docker push gcr.io/project-id/myhaki-agent:tag
</pre>
</div>
- **Auto-Deployment:**  
<div class="api-block">
<pre class="api-dark">
gcloud run deploy --image gcr.io/project-id/myhaki-agent:tag --platform managed --allow-unauthenticated --port 8080
</pre>
</div>
<hr>
<h2>Secrets Access at Runtime</h2>
<div class="api-block">
<pre class="api-dark">
from google.cloud import secretmanager
<p>client = secretmanager.SecretManagerServiceClient()
response = client.access_secret_version(name=&quot;projects/project-id/secrets/gemini-key/versions/latest&quot;)
api_key = response.payload.data.decode(&quot;UTF-8&quot;)</p>
</pre>
</div>
<ul>
<li><strong>Explanation:</strong> Fetches secrets at runtime to prevent leaks.</li>
</ul>
<hr>
<h2>Health and Monitoring</h2>
<ul>
<li><strong>Dockerfile Health Check:</strong></li>
</ul>
<div class="api-block">
<pre class="api-dark">
HEALTHCHECK CMD curl --fail http://localhost:8080/ || exit 1
</pre>
</div>
<ul>
<li><strong>Explanation:</strong> Periodically probes the FastAPI endpoint for readiness; integrates with Google Cloud Monitoring alerts.</li>
</ul>
<hr>
